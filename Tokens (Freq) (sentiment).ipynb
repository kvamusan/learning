{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38e6e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96aa8f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c66bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"This paper examines the complex relationship between language, human cognition, and artificial intelligence...\n",
    "...model of Speech Act theory.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7296d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    # Classify polarity as positive, negative, or neutral\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb6e0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_result = analyze_sentiment(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af5e2bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentiment: {sentiment_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a544172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequency Analysis:\n",
      ",: 35\n",
      "the: 30\n",
      ".: 29\n",
      "to: 24\n",
      "and: 21\n",
      "of: 16\n",
      "in: 13\n",
      "AI: 11\n",
      "human: 8\n",
      "a: 8\n",
      "The: 7\n",
      "emotional: 6\n",
      "emotions: 6\n",
      "understand: 5\n",
      "chatbots: 5\n",
      "conversations: 5\n",
      "needs: 5\n",
      "that: 5\n",
      "customers: 5\n",
      ":: 4\n",
      "It: 4\n",
      "(: 4\n",
      "): 4\n",
      "it: 4\n",
      "interactions: 4\n",
      "’: 4\n",
      "chatbot: 4\n",
      "act: 4\n",
      "context: 4\n",
      "intelligence: 3\n",
      "involved: 3\n",
      "genuinely: 3\n",
      "has: 3\n",
      "have: 3\n",
      "by: 3\n",
      "Gen: 3\n",
      "mental: 3\n",
      "psychological: 3\n",
      "study: 3\n",
      "human-human: 3\n",
      "as: 3\n",
      "using: 3\n",
      "expressive: 3\n",
      "commissive: 3\n",
      "request: 3\n",
      "for: 3\n",
      "respond: 3\n",
      "customer: 3\n",
      "acts: 3\n",
      "“: 3\n",
      "provide: 3\n",
      "your: 3\n",
      "”: 3\n",
      "constantly: 3\n",
      "I: 3\n",
      "Language: 2\n",
      "Artificial: 2\n",
      "Intelligence: 2\n",
      "complex: 2\n",
      "between: 2\n",
      "cognitive: 2\n",
      "processes: 2\n",
      "conversation: 2\n",
      "where: 2\n",
      "human-like: 2\n",
      "imitating: 2\n",
      "human-AI: 2\n",
      "how: 2\n",
      "customer-service: 2\n",
      "been: 2\n",
      "well: 2\n",
      "conversational: 2\n",
      "they: 2\n",
      "anger: 2\n",
      "which: 2\n",
      "theory: 2\n",
      "data: 2\n",
      "complaints: 2\n",
      "make: 2\n",
      "directive: 2\n",
      "details: 2\n",
      "each: 2\n",
      "like: 2\n",
      "humans: 2\n",
      "frustration: 2\n",
      "his: 2\n",
      "exchange: 2\n",
      "apologize: 2\n",
      "inconvenience: 2\n",
      "Please: 2\n",
      "order: 2\n",
      "number: 2\n",
      "we: 2\n",
      "'ll: 2\n",
      "look: 2\n",
      "into: 2\n",
      "further: 2\n",
      "but: 2\n",
      "making: 2\n",
      "you: 2\n",
      "more: 2\n",
      "demonstrated: 2\n",
      "Cambridge: 2\n",
      "TConnecting: 1\n",
      "Cognition: 1\n",
      "And: 1\n",
      "Investigating: 1\n",
      "Scope: 1\n",
      "Of: 1\n",
      "Comprehension: 1\n",
      "In: 1\n",
      "Human-AI: 1\n",
      "Customer-Service: 1\n",
      "This: 1\n",
      "paper: 1\n",
      "examines: 1\n",
      "relationship: 1\n",
      "language: 1\n",
      "cognition: 1\n",
      "artificial: 1\n",
      "seeks: 1\n",
      "envisaging: 1\n",
      "future: 1\n",
      "engage: 1\n",
      "Since: 1\n",
      "Generative: 1\n",
      "entered: 1\n",
      "transformational: 1\n",
      "phase: 1\n",
      "via: 1\n",
      "domain: 1\n",
      "Natural: 1\n",
      "Processing: 1\n",
      "NLP: 1\n",
      "human-agent: 1\n",
      "is: 1\n",
      "pertinent: 1\n",
      "examine: 1\n",
      "representatives: 1\n",
      "represented: 1\n",
      "undertake: 1\n",
      "responsibility: 1\n",
      "responding: 1\n",
      "collected: 1\n",
      "corpus: 1\n",
      "dataset: 1\n",
      "encompassing: 1\n",
      "both: 1\n",
      "comparative: 1\n",
      "analysis: 1\n",
      "natural: 1\n",
      "involving: 1\n",
      "investigated: 1\n",
      "able: 1\n",
      "demonstrate: 1\n",
      "understanding: 1\n",
      "basic: 1\n",
      "datasets: 1\n",
      "included: 1\n",
      "distinct: 1\n",
      "sets: 1\n",
      "contexts: 1\n",
      "encompass: 1\n",
      "elements: 1\n",
      "related: 1\n",
      "such: 1\n",
      "sadness: 1\n",
      "joy: 1\n",
      "surprise: 1\n",
      "L2: 1\n",
      "speakers: 1\n",
      "English: 1\n",
      "examined: 1\n",
      "extent: 1\n",
      "models: 1\n",
      "communication: 1\n",
      "Searls: 1\n",
      "model: 1\n",
      "Speech: 1\n",
      "Act: 1\n",
      "shows: 1\n",
      "uses: 1\n",
      "offer: 1\n",
      "sincere: 1\n",
      "apology: 1\n",
      "response: 1\n",
      "promise: 1\n",
      "things: 1\n",
      "right: 1\n",
      "assertive: 1\n",
      "state: 1\n",
      "or: 1\n",
      "affirm: 1\n",
      "nature: 1\n",
      "their: 1\n",
      "observes: 1\n",
      "while: 1\n",
      "tries: 1\n",
      "lack: 1\n",
      "deep: 1\n",
      "For: 1\n",
      "instance: 1\n",
      "one: 1\n",
      "expressed: 1\n",
      "all: 1\n",
      "exchanges: 1\n",
      "only: 1\n",
      "pacified: 1\n",
      "first: 1\n",
      "We: 1\n",
      "any: 1\n",
      "Subsequently: 1\n",
      "failed: 1\n",
      "display: 1\n",
      "Conversely: 1\n",
      "shares: 1\n",
      "same: 1\n",
      "responded: 1\n",
      "positive: 1\n",
      "commitments: 1\n",
      "Though: 1\n",
      "was: 1\n",
      "necessary: 1\n",
      "agent: 1\n",
      "appeased: 1\n",
      "before: 1\n",
      "requests: 1\n",
      "completely: 1\n",
      "'m: 1\n",
      "sorry: 1\n",
      "Let: 1\n",
      "'s: 1\n",
      "work: 1\n",
      "together: 1\n",
      "resolve: 1\n",
      "these: 1\n",
      "errors: 1\n",
      "Can: 1\n",
      "about: 1\n",
      "issues: 1\n",
      "'ve: 1\n",
      "encountered: 1\n",
      "?: 1\n",
      "higher: 1\n",
      "level: 1\n",
      "adaptability: 1\n",
      "recognition: 1\n",
      "AI-customer: 1\n",
      "service: 1\n",
      "show: 1\n",
      "recognized: 1\n",
      "struggled: 1\n",
      "adapt: 1\n",
      "expressions: 1\n",
      "providing: 1\n",
      "patterned: 1\n",
      "formulaic: 1\n",
      "responses: 1\n",
      "perpetually: 1\n",
      "resulted: 1\n",
      "dissatisfaction: 1\n",
      "are: 1\n",
      "facing: 1\n",
      "challenges: 1\n",
      "especially: 1\n",
      "While: 1\n",
      "made: 1\n",
      "substantial: 1\n",
      "improvement: 1\n",
      "technological: 1\n",
      "enhancement: 1\n",
      "truly: 1\n",
      "References: 1\n",
      "Korteling: 1\n",
      "J.: 1\n",
      "E.: 1\n",
      "H: 1\n",
      "van: 1\n",
      "de: 1\n",
      "Boer-Visschedijk: 1\n",
      "GC: 1\n",
      "Blankendaal: 1\n",
      "RAM: 1\n",
      "Boonekamp: 1\n",
      "RC: 1\n",
      "Eikelboom: 1\n",
      "AR: 1\n",
      "2021: 1\n",
      "Human: 1\n",
      "versus: 1\n",
      "Front: 1\n",
      "Artif: 1\n",
      "Intell: 1\n",
      "4:622364.: 1\n",
      "doi: 1\n",
      "10.3389/frai.2021.622364: 1\n",
      "Searle: 1\n",
      "J: 1\n",
      "1986: 1\n",
      "Expression: 1\n",
      "meaning: 1\n",
      "Studies: 1\n",
      "speech: 1\n",
      "University: 1\n",
      "Press: 1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Sample text data (replace this with your actual text)\n",
    "text_data = \"\"\"TConnecting Language, Cognition, And Artificial Intelligence: Investigating The Scope Of Comprehension In Human-AI Customer-Service\n",
    "This paper examines the complex relationship between language, human cognition, and artificial intelligence. It seeks to understand the cognitive processes involved in human conversation, envisaging a future where AI chatbots engage in genuinely human-like conversations. Since Generative AI has entered a transformational phase via the domain of Natural Language Processing (NLP) imitating human-agent in human-AI conversations, it is pertinent to examine how customer-service representatives have been well represented by Gen AI to undertake the responsibility of responding to human emotional, mental, and psychological needs. \n",
    "The study collected a corpus dataset encompassing both human-human and human-AI conversations. It involved a comparative analysis of natural human interactions and interactions involving AI chatbots. It investigated how AI chatbots have been able to demonstrate understanding of basic human emotional, mental, and psychological needs. The datasets included distinct sets of conversational contexts as they encompass conversational elements related to emotions such as sadness, anger, joy, and surprise between L2 speakers of English and AI. It examined the extent to which Gen AI models understand the cognitive processes involved in human communication using Searls’ model of Speech Act theory. \n",
    "The data shows that the AI chatbot uses expressive act to offer a sincere apology as response to customers’ complaints, the commissive act to promise to make things right, the directive act to request for customers’ details and the assertive act to state or affirm the nature of their complaints. The study observes that while each chatbot tries to genuinely respond to customers’ needs like humans, they lack deep emotional intelligence like humans. For instance, in one of the data where a customer expressed anger and frustration in all his exchanges, the AI chatbot only pacified the emotions of the customer in his first exchange using the expressive, directive and commissive acts “We apologize for any inconvenience. Please provide your order number, and we'll look into it”. Subsequently, the chatbot failed to respond to further display of emotions but constantly making request “I understand. Please provide your order number, and we'll look into it”. Conversely, the human-human conversation that shares the same context responded well to customers emotions in each exchange by constantly using expressive and commissive acts to apologize and make positive commitments. Though a request was necessary, the human agent constantly appeased the emotions of the customer before making further requests “I completely understand your frustration, and I'm sorry for the inconvenience. Let's work together to resolve these errors. Can you provide more details about the issues you've encountered?” \n",
    "The human-human conversations have demonstrated a higher level of adaptability, context recognition and emotional intelligence. AI-customer service interactions show that the chatbots recognized the context but struggled to adapt to the emotional expressions of the customers by providing patterned and formulaic responses, which perpetually resulted in dissatisfaction. The study has demonstrated that AI chatbots are facing challenges in genuinely imitating human-like conversations, especially in the context of complex emotions. While Gen AI has made substantial improvement in customer-service interactions, it needs more technological enhancement to truly understand and respond to human emotional, mental, and psychological needs.\n",
    "\n",
    "References\n",
    "Korteling J. E. (H), van de Boer-Visschedijk GC, Blankendaal RAM, Boonekamp RC and \n",
    "Eikelboom AR (2021) Human versus Artificial Intelligence. Front. Artif. Intell. 4:622364. \n",
    "doi: 10.3389/frai.2021.622364\n",
    "Searle, J. (1986). Expression and meaning: Studies in the theory of speech acts. Cambridge: \n",
    "Cambridge University Press.\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(text_data)\n",
    "\n",
    "# Calculate word frequency distribution\n",
    "freq_dist = FreqDist(words)\n",
    "\n",
    "# Display the most common words and their frequencies\n",
    "print(\"Word Frequency Analysis:\")\n",
    "for word, frequency in freq_dist.most_common():\n",
    "    print(f\"{word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4ab200e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\amusa\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Frequency Analysis:\n",
      "the: 58220\n",
      ",: 53665\n",
      ".: 50901\n",
      "of: 35949\n",
      "to: 33981\n",
      "in: 26447\n",
      "said: 25222\n",
      "and: 25015\n",
      "a: 23448\n",
      "mln: 18012\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the Reuters corpus\n",
    "nltk.download('reuters')\n",
    "corpus = reuters.raw()\n",
    "\n",
    "# Tokenize the text\n",
    "words = word_tokenize(corpus)\n",
    "\n",
    "# Calculate word frequency distribution\n",
    "freq_dist = FreqDist(words)\n",
    "\n",
    "# Display the most common words and their frequencies\n",
    "print(\"Word Frequency Analysis:\")\n",
    "for word, frequency in freq_dist.most_common(10):\n",
    "    print(f\"{word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d4d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
